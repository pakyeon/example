{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model_name = \"google/gemma-3-4b-it\"\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # 훈련 시 캐싱 비활성화\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name, use_fast=True)\n",
    "tokenizer = processor.tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unsloth import FastModel\n",
    "\n",
    "# model, tokenizer = FastModel.from_pretrained(\n",
    "#     model_name=\"unsloth/gemma-3-4b-it\",\n",
    "#     max_seq_length=2048,\n",
    "#     load_in_4bit=True,\n",
    "#     load_in_8bit=False,\n",
    "#     full_finetuning=False,\n",
    "#     dtype=torch.bfloat16,\n",
    "#     device_map=\"auto\",\n",
    "#     # token = \"hf_...\",\n",
    "# )\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token\n",
    "# tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 구성을 위한 설정 (인과적 언어 모델링용)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],  # 타겟 모듈 지정\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",  # 작업 유형 설정\n",
    "    # use_rslora=True,\n",
    "    # use_dora=True,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FastModel.get_peft_model(\n",
    "#     model,\n",
    "#     finetune_vision_layers=False,\n",
    "#     finetune_language_layers=True,\n",
    "#     finetune_attention_modules=True,\n",
    "#     finetune_mlp_modules=True,\n",
    "#     r=8,\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0,\n",
    "#     bias=\"none\",\n",
    "#     random_state=3407,\n",
    "#     use_gradient_checkpointing=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Yeongi/Bespoke-Stratos-3.65k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ===== 답변 추출 함수 =====\n",
    "# def extract_boxed(s):\n",
    "#     start = s.find(\"boxed{\")\n",
    "#     if start == -1:\n",
    "#         return None\n",
    "#     start_index = start + len(\"boxed{\")\n",
    "#     count = 1  # 처음의 '{'에 대해 1로 시작\n",
    "#     i = start_index\n",
    "#     while i < len(s):\n",
    "#         if s[i] == \"{\":\n",
    "#             count += 1\n",
    "#         elif s[i] == \"}\":\n",
    "#             count -= 1\n",
    "#             if count == 0:\n",
    "#                 return s[start_index:i]\n",
    "#         i += 1\n",
    "#     return None\n",
    "\n",
    "\n",
    "# def extract_aime_answer(response):\n",
    "#     # 박스 형식 및 숫자 직접 매칭\n",
    "#     # 모든 \\boxed{} 패턴을 찾아 마지막 항목 선택\n",
    "#     boxed_matches = extract_boxed(response)\n",
    "#     if boxed_matches:\n",
    "#         raw_answer = boxed_matches\n",
    "#     else:\n",
    "#         patterns = [\n",
    "#             r\"<\\|begin_of_solution\\|>.*?```python(.*?)```.*?<\\|end_of_solution\\|>\",\n",
    "#             r\"ANSWER\\s*:\\s*(\\d+)\",  # 간단한 숫자 형식\n",
    "#             r\"final answer is:\\s*(\\d+)\",  # 대체 표현\n",
    "#             r\"<\\|begin_of_solution\\|>(.*?)<\\|end_of_solution\\|>\",\n",
    "#         ]\n",
    "#         for pattern in patterns:\n",
    "#             match = re.search(pattern, response, re.DOTALL)\n",
    "#             if match:\n",
    "#                 raw_answer = match.group(1).strip()\n",
    "#                 break\n",
    "#         else:\n",
    "#             return None\n",
    "#     return raw_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_dataset = dataset.map(\n",
    "#     lambda x: {\n",
    "#         \"prompt\": [\n",
    "#             {\"role\": \"system\", \"content\": x[\"system\"][0]},\n",
    "#             {\"role\": \"user\", \"content\": x[\"question\"]},\n",
    "#         ],\n",
    "#         \"answer\": extract_aime_answer(x[\"response\"]),\n",
    "#     },\n",
    "#     num_proc=6,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 토큰 수를 저장할 리스트\n",
    "# token_lengths = []\n",
    "\n",
    "# # 데이터셋의 모든 텍스트에 대해 토큰 수 계산\n",
    "# for idx, text in enumerate(filtered_dataset):\n",
    "#     num_tokens = len(tokenizer.tokenizer(text['system'])['input_ids']) + len(tokenizer.tokenizer(text['question'])['input_ids']) + len(tokenizer.tokenizer(text['response'])['input_ids'])\n",
    "#     token_lengths.append((num_tokens, idx))\n",
    "\n",
    "# # 최대 및 최소 토큰 수를 가진 데이터의 인덱스 찾기\n",
    "# max_token_info = max(token_lengths, key=lambda x: x[0])  # 토큰 수가 최대인 데이터\n",
    "# min_token_info = min(token_lengths, key=lambda x: x[0])  # 토큰 수가 최소인 데이터\n",
    "\n",
    "# # 최대 및 최소 토큰 수를 가진 데이터 출력\n",
    "# max_tokens, max_idx = max_token_info\n",
    "# min_tokens, min_idx = min_token_info\n",
    "\n",
    "# print(f\"최대 토큰 수: {max_tokens}\")\n",
    "# print(f\"최대 토큰 수를 가진 데이터: {filtered_dataset[max_idx]}, idx: {max_idx}\")\n",
    "# print(f\"최소 토큰 수: {min_tokens}\")\n",
    "# print(f\"최소 토큰 수를 가진 데이터: {filtered_dataset[min_idx]}, idx: {min_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset):\n",
    "    # 입력 텍스트 준비 (시스템 프롬프트 + 질문)\n",
    "    inputs = [\n",
    "        f\"{system}\\n\\n{question}\"\n",
    "        for system, question in zip(dataset[\"system\"], dataset[\"question\"])\n",
    "    ]\n",
    "\n",
    "    # 출력 텍스트 준비 (응답)\n",
    "    outputs = [f\"{response}{tokenizer.eos_token}\" for response in dataset[\"response\"]]\n",
    "\n",
    "    # 입력 토큰화\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=2048, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # 라벨(출력) 토큰화\n",
    "    labels = tokenizer(\n",
    "        outputs,\n",
    "        max_length=2048,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    # 라벨 ID를 모델 입력에 추가\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function, batched=True, remove_columns=dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()  # 반드시 추가!\n",
    "model.enable_input_require_grads()  # 그래디언트 계산 강제 활성화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고: https://huggingface.co/blog/open-r1/update-3\n",
    "output_dir = os.path.join(\"outputs\", model_name)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=processor.tokenizer,  # 멀티모달\n",
    "    # tokenizer=tokenizer, # Only 언어모델\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=None,\n",
    "    args=SFTConfig(\n",
    "        bf16=True,\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=4,\n",
    "        gradient_checkpointing=True,\n",
    "        max_grad_norm=0.2,\n",
    "        warmup_ratio=0.1,\n",
    "        weight_decay=0.01,\n",
    "        num_train_epochs=1,\n",
    "        # max_steps = 50, # test only\n",
    "        learning_rate=2e-5,\n",
    "        logging_steps=1,\n",
    "        optim=\"adamw_torch_fused\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_steps=10,\n",
    "        save_total_limit=10,\n",
    "        seed=3407,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=\"gemma3-12b-lora-sft\",\n",
    "        label_names=[\"labels\"],\n",
    "        # packing=True,\n",
    "        output_dir=output_dir,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
