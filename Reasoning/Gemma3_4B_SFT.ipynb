{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 03-27 03:32:17 [__init__.py:239] Automatically detected platform cuda.\n",
      "Unsloth: LoRA, QLoRA and full finetuning all not selected. Switching to QLoRA.\n",
      "==((====))==  Unsloth 2025.3.14: Fast Gemma3 patching. Transformers: 4.51.0.dev0. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=\"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=False,\n",
    "    load_in_8bit=False,\n",
    "    full_finetuning=False,\n",
    "    dtype=torch.bfloat16,\n",
    "    # token = \"hf_...\",\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.language_model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers=False,\n",
    "    finetune_language_layers=True,\n",
    "    finetune_attention_modules=True,\n",
    "    finetune_mlp_modules=True,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    random_state=3407,\n",
    "    use_gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingFaceH4/Bespoke-Stratos-17k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_restruction(dataset):\n",
    "    system = dataset[\"system\"]\n",
    "    question = dataset[\"conversations\"][0][\"value\"]\n",
    "    response = dataset[\"conversations\"][1][\"value\"]\n",
    "\n",
    "    return {\"system\": system, \"question\": question, \"response\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "restructured_dataset = dataset.map(\n",
    "    format_restruction,\n",
    "    remove_columns=dataset.column_names,  # ê¸°ì¡´ ì»¬ëŸ¼ ì œê±°\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_long_tokens(dataset):\n",
    "    return (\n",
    "        len(tokenizer.tokenizer(dataset[\"system\"])[\"input_ids\"])\n",
    "        + len(tokenizer.tokenizer(dataset[\"question\"])[\"input_ids\"])\n",
    "        + len(tokenizer.tokenizer(dataset[\"response\"])[\"input_ids\"])\n",
    "        <= 2044\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995f0f744e734abca0536cb2f874bbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_dataset = restructured_dataset.filter(filter_long_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # í† í° ìˆ˜ë¥¼ ì €ìž¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "# token_lengths = []\n",
    "\n",
    "# # ë°ì´í„°ì…‹ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ì— ëŒ€í•´ í† í° ìˆ˜ ê³„ì‚°\n",
    "# for idx, text in enumerate(filtered_dataset):\n",
    "#     num_tokens = len(tokenizer.tokenizer(text['system'])['input_ids']) + len(tokenizer.tokenizer(text['question'])['input_ids']) + len(tokenizer.tokenizer(text['response'])['input_ids'])\n",
    "#     token_lengths.append((num_tokens, idx))\n",
    "\n",
    "# # ìµœëŒ€ ë° ìµœì†Œ í† í° ìˆ˜ë¥¼ ê°€ì§„ ë°ì´í„°ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
    "# max_token_info = max(token_lengths, key=lambda x: x[0])  # í† í° ìˆ˜ê°€ ìµœëŒ€ì¸ ë°ì´í„°\n",
    "# min_token_info = min(token_lengths, key=lambda x: x[0])  # í† í° ìˆ˜ê°€ ìµœì†Œì¸ ë°ì´í„°\n",
    "\n",
    "# # ìµœëŒ€ ë° ìµœì†Œ í† í° ìˆ˜ë¥¼ ê°€ì§„ ë°ì´í„° ì¶œë ¥\n",
    "# max_tokens, max_idx = max_token_info\n",
    "# min_tokens, min_idx = min_token_info\n",
    "\n",
    "# print(f\"ìµœëŒ€ í† í° ìˆ˜: {max_tokens}\")\n",
    "# print(f\"ìµœëŒ€ í† í° ìˆ˜ë¥¼ ê°€ì§„ ë°ì´í„°: {filtered_dataset[max_idx]}, idx: {max_idx}\")\n",
    "# print(f\"ìµœì†Œ í† í° ìˆ˜: {min_tokens}\")\n",
    "# print(f\"ìµœì†Œ í† í° ìˆ˜ë¥¼ ê°€ì§„ ë°ì´í„°: {filtered_dataset[min_idx]}, idx: {min_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset):\n",
    "    # ìž…ë ¥ í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ì§ˆë¬¸)\n",
    "    inputs = [\n",
    "        f\"{system}\\n\\n{question}\"\n",
    "        for system, question in zip(dataset[\"system\"], dataset[\"question\"])\n",
    "    ]\n",
    "\n",
    "    # ì¶œë ¥ í…ìŠ¤íŠ¸ ì¤€ë¹„ (ì‘ë‹µ)\n",
    "    outputs = [f\"{response}\" for response in dataset[\"response\"]]\n",
    "\n",
    "    # ìž…ë ¥ í† í°í™”\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=2048, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # ë¼ë²¨(ì¶œë ¥) í† í°í™”\n",
    "    labels = tokenizer(outputs, max_length=2048, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # ë¼ë²¨ IDë¥¼ ëª¨ë¸ ìž…ë ¥ì— ì¶”ê°€\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030b7ba038a248748b02c17cf53be8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = filtered_dataset.map(\n",
    "    preprocess_function, batched=True, remove_columns=restructured_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3641\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=None,\n",
    "    args=SFTConfig(\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        gradient_checkpointing=True,\n",
    "        warmup_ratio=0.1,\n",
    "        num_train_epochs=1,\n",
    "        # max_steps = 50, # test only\n",
    "        learning_rate=2.0e-5,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_steps=50,\n",
    "        save_total_limit=50,\n",
    "        seed=3407,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=\"gemma3-4b-peft-lora-sft\",\n",
    "        # packing=True, # unsloth paking ë²„ê·¸ë¡œ ì¸í•œ ë¹„í™œì„±í™”\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train(resume_from_checkpoint=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
