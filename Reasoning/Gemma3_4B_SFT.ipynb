{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 03-27 03:32:17 [__init__.py:239] Automatically detected platform cuda.\n",
      "Unsloth: LoRA, QLoRA and full finetuning all not selected. Switching to QLoRA.\n",
      "==((====))==  Unsloth 2025.3.14: Fast Gemma3 patching. Transformers: 4.51.0.dev0. vLLM: 0.8.2.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4070 Ti SUPER. Num GPUs = 1. Max memory: 15.992 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=\"unsloth/gemma-3-4b-it\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=False,\n",
    "    load_in_8bit=False,\n",
    "    full_finetuning=False,\n",
    "    dtype=torch.bfloat16,\n",
    "    # token = \"hf_...\",\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.language_model` require gradients\n"
     ]
    }
   ],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers=False,\n",
    "    finetune_language_layers=True,\n",
    "    finetune_attention_modules=True,\n",
    "    finetune_mlp_modules=True,\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    random_state=3407,\n",
    "    use_gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingFaceH4/Bespoke-Stratos-17k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_restruction(dataset):\n",
    "    system = dataset[\"system\"]\n",
    "    question = dataset[\"conversations\"][0][\"value\"]\n",
    "    response = dataset[\"conversations\"][1][\"value\"]\n",
    "\n",
    "    return {\"system\": system, \"question\": question, \"response\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "restructured_dataset = dataset.map(\n",
    "    format_restruction,\n",
    "    remove_columns=dataset.column_names,  # 기존 컬럼 제거\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_long_tokens(dataset):\n",
    "    return (\n",
    "        len(tokenizer.tokenizer(dataset[\"system\"])[\"input_ids\"])\n",
    "        + len(tokenizer.tokenizer(dataset[\"question\"])[\"input_ids\"])\n",
    "        + len(tokenizer.tokenizer(dataset[\"response\"])[\"input_ids\"])\n",
    "        <= 2044\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995f0f744e734abca0536cb2f874bbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_dataset = restructured_dataset.filter(filter_long_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 토큰 수를 저장할 리스트\n",
    "# token_lengths = []\n",
    "\n",
    "# # 데이터셋의 모든 텍스트에 대해 토큰 수 계산\n",
    "# for idx, text in enumerate(filtered_dataset):\n",
    "#     num_tokens = len(tokenizer.tokenizer(text['system'])['input_ids']) + len(tokenizer.tokenizer(text['question'])['input_ids']) + len(tokenizer.tokenizer(text['response'])['input_ids'])\n",
    "#     token_lengths.append((num_tokens, idx))\n",
    "\n",
    "# # 최대 및 최소 토큰 수를 가진 데이터의 인덱스 찾기\n",
    "# max_token_info = max(token_lengths, key=lambda x: x[0])  # 토큰 수가 최대인 데이터\n",
    "# min_token_info = min(token_lengths, key=lambda x: x[0])  # 토큰 수가 최소인 데이터\n",
    "\n",
    "# # 최대 및 최소 토큰 수를 가진 데이터 출력\n",
    "# max_tokens, max_idx = max_token_info\n",
    "# min_tokens, min_idx = min_token_info\n",
    "\n",
    "# print(f\"최대 토큰 수: {max_tokens}\")\n",
    "# print(f\"최대 토큰 수를 가진 데이터: {filtered_dataset[max_idx]}, idx: {max_idx}\")\n",
    "# print(f\"최소 토큰 수: {min_tokens}\")\n",
    "# print(f\"최소 토큰 수를 가진 데이터: {filtered_dataset[min_idx]}, idx: {min_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset):\n",
    "    # 입력 텍스트 준비 (시스템 프롬프트 + 질문)\n",
    "    inputs = [\n",
    "        f\"{system}\\n\\n{question}\"\n",
    "        for system, question in zip(dataset[\"system\"], dataset[\"question\"])\n",
    "    ]\n",
    "\n",
    "    # 출력 텍스트 준비 (응답)\n",
    "    outputs = [f\"{response}\" for response in dataset[\"response\"]]\n",
    "\n",
    "    # 입력 토큰화\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=2048, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # 라벨(출력) 토큰화\n",
    "    labels = tokenizer(outputs, max_length=2048, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    # 라벨 ID를 모델 입력에 추가\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030b7ba038a248748b02c17cf53be8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3641 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = filtered_dataset.map(\n",
    "    preprocess_function, batched=True, remove_columns=restructured_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3641\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=None,\n",
    "    args=SFTConfig(\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        gradient_checkpointing=True,\n",
    "        warmup_ratio=0.1,\n",
    "        num_train_epochs=1,\n",
    "        # max_steps = 50, # test only\n",
    "        learning_rate=2.0e-5,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_steps=50,\n",
    "        save_total_limit=50,\n",
    "        seed=3407,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=\"gemma3-4b-peft-lora-sft\",\n",
    "        # packing=True, # unsloth paking 버그로 인한 비활성화\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train(resume_from_checkpoint=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
