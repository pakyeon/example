{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3ef3ff3bf14777825fc64dd241770f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = Gemma3ForConditionalGeneration.from_pretrained(\n",
    "    \"google/gemma-3-12b-it\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # 훈련 시 캐싱 비활성화\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"google/gemma-3-12b-it\", use_fast=True)\n",
    "processor.tokenizer.pad_token = processor.tokenizer.eos_token\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unsloth import FastModel\n",
    "\n",
    "\n",
    "# model, tokenizer = FastModel.from_pretrained(\n",
    "#     model_name=\"unsloth/gemma-3-4b-it\",\n",
    "#     max_seq_length=2048,\n",
    "#     load_in_4bit=False,\n",
    "#     load_in_8bit=False,\n",
    "#     full_finetuning=False,\n",
    "#     dtype=torch.bfloat16,\n",
    "#     # token = \"hf_...\",\n",
    "# )\n",
    "\n",
    "# tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 구성을 위한 설정 (인과적 언어 모델링용)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # 타겟 모듈 지정\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",  # 작업 유형 설정\n",
    "    # use_rslora=True,\n",
    "    use_dora=True,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FastModel.get_peft_model(\n",
    "#     model,\n",
    "#     finetune_vision_layers=False,\n",
    "#     finetune_language_layers=True,\n",
    "#     finetune_attention_modules=True,\n",
    "#     finetune_mlp_modules=True,\n",
    "#     r=8,\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0,\n",
    "#     bias=\"none\",\n",
    "#     random_state=3407,\n",
    "#     use_gradient_checkpointing=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"HuggingFaceH4/Bespoke-Stratos-17k\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_restruction(dataset):\n",
    "    system = dataset[\"system\"]\n",
    "    question = dataset[\"conversations\"][0][\"value\"]\n",
    "    response = dataset[\"conversations\"][1][\"value\"]\n",
    "\n",
    "    return {\"system\": system, \"question\": question, \"response\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "restructured_dataset = dataset.map(\n",
    "    format_restruction,\n",
    "    remove_columns=dataset.column_names,  # 기존 컬럼 제거\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_long_tokens(dataset):\n",
    "    return (\n",
    "        len(processor.tokenizer.tokenize(dataset[\"system\"]))\n",
    "        + len(processor.tokenizer.tokenize(dataset[\"question\"]))\n",
    "        + len(processor.tokenizer.tokenize(dataset[\"response\"]))\n",
    "        <= 2044\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def filter_long_tokens(dataset):\n",
    "#     return (\n",
    "#         len(tokenizer.tokenizer(dataset[\"system\"])[\"input_ids\"])\n",
    "#         + len(tokenizer.tokenizer(dataset[\"question\"])[\"input_ids\"])\n",
    "#         + len(tokenizer.tokenizer(dataset[\"response\"])[\"input_ids\"])\n",
    "#         <= 2044\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = restructured_dataset.filter(\n",
    "    filter_long_tokens, num_proc=4, desc=\"Filtering dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 토큰 수를 저장할 리스트\n",
    "# token_lengths = []\n",
    "\n",
    "# # 데이터셋의 모든 텍스트에 대해 토큰 수 계산\n",
    "# for idx, text in enumerate(filtered_dataset):\n",
    "#     num_tokens = len(tokenizer.tokenizer(text['system'])['input_ids']) + len(tokenizer.tokenizer(text['question'])['input_ids']) + len(tokenizer.tokenizer(text['response'])['input_ids'])\n",
    "#     token_lengths.append((num_tokens, idx))\n",
    "\n",
    "# # 최대 및 최소 토큰 수를 가진 데이터의 인덱스 찾기\n",
    "# max_token_info = max(token_lengths, key=lambda x: x[0])  # 토큰 수가 최대인 데이터\n",
    "# min_token_info = min(token_lengths, key=lambda x: x[0])  # 토큰 수가 최소인 데이터\n",
    "\n",
    "# # 최대 및 최소 토큰 수를 가진 데이터 출력\n",
    "# max_tokens, max_idx = max_token_info\n",
    "# min_tokens, min_idx = min_token_info\n",
    "\n",
    "# print(f\"최대 토큰 수: {max_tokens}\")\n",
    "# print(f\"최대 토큰 수를 가진 데이터: {filtered_dataset[max_idx]}, idx: {max_idx}\")\n",
    "# print(f\"최소 토큰 수: {min_tokens}\")\n",
    "# print(f\"최소 토큰 수를 가진 데이터: {filtered_dataset[min_idx]}, idx: {min_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(dataset):\n",
    "    # 입력 텍스트 준비 (시스템 프롬프트 + 질문)\n",
    "    inputs = [\n",
    "        f\"{system}\\n\\n{question}\"\n",
    "        for system, question in zip(dataset[\"system\"], dataset[\"question\"])\n",
    "    ]\n",
    "\n",
    "    # 출력 텍스트 준비 (응답)\n",
    "    outputs = [f\"{response}\" for response in dataset[\"response\"]]\n",
    "\n",
    "    # 입력 토큰화\n",
    "    model_inputs = processor.tokenizer(\n",
    "        inputs, max_length=2048, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # 라벨(출력) 토큰화\n",
    "    labels = processor.tokenizer(\n",
    "        outputs, max_length=2048, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # 라벨 ID를 모델 입력에 추가\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = filtered_dataset.map(\n",
    "    preprocess_function, batched=True, remove_columns=restructured_dataset.column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()  # 반드시 추가!\n",
    "model.enable_input_require_grads()  # 그래디언트 계산 강제 활성화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참고: https://huggingface.co/blog/open-r1/update-3\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=processor.tokenizer,  # 멀티모달\n",
    "    # tokenizer=tokenizer, # Only 언어모델\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=None,\n",
    "    args=SFTConfig(\n",
    "        bf16=True,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=8,\n",
    "        gradient_checkpointing=True,\n",
    "        max_grad_norm=0.2,\n",
    "        warmup_ratio=0.1,\n",
    "        num_train_epochs=1,\n",
    "        # max_steps = 50, # test only\n",
    "        learning_rate=2e-5,\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        save_steps=50,\n",
    "        save_total_limit=50,\n",
    "        seed=3407,\n",
    "        report_to=\"wandb\",\n",
    "        run_name=\"gemma3-12b-lora\",\n",
    "        label_names=[\"labels\"],\n",
    "        # packing=True, # unsloth paking 버그로 인한 비활성화\n",
    "        output_dir=\"outputs\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhiyo2044\u001b[0m (\u001b[33mhiyo2044-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/hiyo2044/Project/example/Reasoning/wandb/run-20250327_183802-bma72eey</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hiyo2044-/huggingface/runs/bma72eey' target=\"_blank\">gemma3-12b-lora</a></strong> to <a href='https://wandb.ai/hiyo2044-/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hiyo2044-/huggingface' target=\"_blank\">https://wandb.ai/hiyo2044-/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hiyo2044-/huggingface/runs/bma72eey' target=\"_blank\">https://wandb.ai/hiyo2044-/huggingface/runs/bma72eey</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='228' max='228' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [228/228 3:09:45, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>25.071900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>24.291900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>22.450600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>20.070400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>17.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>15.561800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>12.969300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>9.682500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>6.600800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.372900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.454700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>3.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.663400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.574500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.553300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.487200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.334500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.353600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.136900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.397700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.384400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.156000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=228, training_loss=8.390843784599973, metrics={'train_runtime': 11491.2618, 'train_samples_per_second': 0.318, 'train_steps_per_second': 0.02, 'total_flos': 2.507715295838208e+17, 'train_loss': 8.390843784599973})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
